\section{Introduction}

Social interactions are common, but they rarely take place in isolation. Conversations and other group interactions occur on busy streets, in crowded cafes, in large conference halls, and many other types of social gatherings. In these cases, before a computer vision system can \emph{recognize} distinctive group interactions, it must first \emph{detect} them by distinguishing between participants and by-standers, and by localizing these participants in space and time. This paper addresses this spatio-temporal detection problem for cases in which the actors in a large gathering can be approximately detected and tracked.

We approach this as a matching problem. Given an exemplar video of a group interaction that involves a small handful of people, we aim to detect and localize instances of similar interactions within a long video of a larger gathering. 
Specifically, we accept as input a video of a group of individuals, within which may occur an interesting interactive activity involving a handful of participants over an unknown period of time. Our approach localizes interactions of potential interest by explicitly identifying the participants within the group and estimating the beginning and end of their interaction. It also classifies these detected interactions into several categories based on the exemplar videos. Therefore, our  contribution is an approach that goes beyond most previous efforts \cite{Hongeng:act,Gong:act,Hakeem:act,McCowan:meeting,Ni:group,Choi:recogtrack,Intille:act,Vlad:group} which only recognize the activity of interest and assume that the activity occupies the entire length of the video and all humans appearing in the video are participants of the activity. Our appraoch also advances the activity detection problem from only considering a single individual \cite{Ke:detection,Yuan:detection,Shechtman:detection,Hu:detection,Laptev:detection,Duchenne:detection} into considering multi-people behaviors.

\begin{figure}[t]
\begin{center}
\includegraphics[width=\columnwidth]{voting.pdf}
\end{center}
\caption{Detecting and localizing interactions in social clutter. Given an exemplar video of an $N$-person social interaction, we seek to find similar interactions in a long input video with $M>N$ approximately-tracked actors. For each temporal frame in the exemplar, the $N$ best-matching participants are identified separately in each input video frame, and the matches are assigned scores. Matching scores are accumulated over time through a voting scheme that is insensitive to tracking errors and changes in action rates, and this produces a spatial localization of the $N$ participating actors. Their interaction is then localized in time using an efficient branch-and-bound search.}
\label{diagram}
\end{figure}

Given an input video, our approach attempts to match a subset of the observed agents to each of a collection of annotated social interaction exemplars in a database. As depicted in Figure \ref{diagram}, we provide a ranked list of annotated exemplars, with each exemplar associated with the best-matching `roster' of participating individuals in the input video, an estimate of the temporal extent of this best-matching interaction, and a companion score that indicates the similarity between this best-matching interaction and the exemplar. Our output allows classification, for example, by inferring for each interaction detected in the input a class label based on the labels of top-ranked exemplars. An advantage of our approach is its generality: Social interactions are represented using a sequence of ensembles of per-person features to encode appearance and/or motion of each participant, along with a separate sequence of ensembles of pair-wise contextual features to encode relative appearance and/or motion of each participant with respect to all others. The overall approach includes an efficient ensemble matching mechanism to compare the set of descriptors from an exemplar and the set of descriptors from the input, and learns the optimal matching parameters discriminatively. These two efforts enable both a Hough voting with dual accumulator arrays to robustly distinguish the participants from all and a Branch-and-Bound search for temporal localization. Therefore, it can be applied in most scenarios that involve agents interacting within a larger group: We evaluate the approach using three very different datasets. One is the UT-Interaction Dataset~\cite{Ryoo:group}, one is a new collection of videos from an `interactive classroom' (e.g.~\cite{Crouch:PI}), where students self-organize into small groups and engage in discussion, and the other is the Caltech Resident-Intruder Mouse dataset \cite{CRIM13} by which we show that our approach is applicable to non-human scenario and directly accommodates traditional tasks without changes.

Our work builds on a small collection of related work: \cite{Li:segmentation} identifies the relevant motion from a clutter, \cite{Cristani:discovery} detects pre-defined geometric configuration of individual poses, and \cite{Lan:retrieval} retrieves individual actions in the context of surrounding humans. Those more similar to ours are \cite{Ryoo:group,Amer:group}: The former recognizes and temporally localizes interactions, and the latter implicitly infers participants from a generative model for a set of histograms of pose-coded detections computed at a dense spatio-temporal grid. We empirically compare our approach with \cite{Ryoo:group,Amer:group} in Section \ref{expall}.