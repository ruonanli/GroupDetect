% !TEX root =  GroupDet.tex


\vspace{0.05in}\noindent\noindent\textbf{UT-Interaction Dataset.} For comparison to the state-of-art, we evaluate our approach on the UT-Interaction dataset~\cite{Ryoo:group}. We follow the protocol defined in previous work~\cite{Ryoo:group,Amer:group}: 20\% of available interaction annotations are used as exemplars for training, and the remaining (non-annotated) sequences are used for testing. For individual descriptors we use 32-dimensional histogram of space-time features~\cite{Dollar:STIP}, and for pairwise descriptors we use 32-dimensional differences of optical flow histograms to encode relative motion (more details in supplemental material). To obtain agent tracks in input sequences we use a per-frame detector~\cite{Pedro:detect} with a simple between-frame association rule.

\begin{table}[ht]
\vspace{-5pt}
\centering \caption{Classification accuracies and false positive (FP) rates for the proposed method and the baselines on UT-Interaction dataset.}
\footnotesize{
\begin{tabular}{|c||c|c|}
\hline   & Accuracy (\cite{Ryoo:group}, \cite{Amer:group}, ours) & FP Rate (\cite{Ryoo:group}, \cite{Amer:group}, ours) \\
\hline Hug &  (0.875, 0.904, \underline{1.00}) & (0.075, 0.055, \underline{0.00}) \\
\hline Kick &  (0.750, 0.775, \underline{0.875})  & (0.138, 0.108, \underline{0.063})\\
\hline Point & (0.625, 0.663,  \underline{0.750}) & (\underline{0.025}, \underline{0.025}, 0.088)\\
\hline Punch & (0.500, 0.632, \underline{0.750})  & (0.201, 0.154,  \underline{0.138})\\
\hline Push & (0.750, \underline{0.782} , 0.750)  & (0.125, \underline{0.101},  0.138)\\
\hline Shake Hands &  (0.750, 0.789, \underline{1.00}) & (0.088, 0.060, \underline{0.00}) \\
\hline\hline Average &  (0.708, 0.758, \underline{0.854})  & (0.108, 0.083,  \underline{0.071})\\
\hline 
\end{tabular}
}
\label{UTaccuFP}
\vspace{-5pt}
\end{table}

Table~\ref{UTaccuFP} compares recognition accuracy and false-alarm rates to those of previous work~\cite{Ryoo:group,Amer:group}. For our system, we consider one database exemplar at a time, compute its maximal response over the input video, and claim a true positive only when both the class-label and the identified participants are simultaneously correct. Otherwise a false positive is indicated for that exemplar class. By these measures, our approach provides improved accuracy and competitive false positive rates. Next we study detection in terms both temporal localization and participant identification. For temporal localization, we follow the protocol of~\cite{Amer:group} by indicating a true-positive when there is correct classification and more than a $50\%$ ratio between the intersection and union of the estimated temporal interval and the ground-truth. We achieve a slightly smaller area under ROC curve than the two baselines, as shown in Table ~\ref{UTarea}, but point out that differences are hard to interpret because the temporal boundaries are somewhat ambiguous for the consecutively-executed interactions in the dataset. To assess participant identification, we enforce a stricter true-positive criterion that requires $100\%$ correct identification instead of the $50\%$ value used in~\cite{Amer:group} and our system still outperforms the method of \cite{Amer:group}. We attribute this to the fact that we explicitly discriminate interactions and participants in the form of tracks of bounding boxes, while \cite{Amer:group} does not do so but simply explains an input using a non-discriminative generative model.

\begin{table}[ht]
\vspace{-5pt}
\centering \caption{Area under ROC curve for the proposed method and the baselines on UT-Interaction dataset.}
\footnotesize{
\begin{tabular}{|c|c|c|c|}
\hline   & \cite{Ryoo:group} &  \cite{Amer:group}  &   ours \\
\hline Temporal Localization &  0.91 & \underline{0.94} &  0.89\\
\hline Participants Identification &  N/A & 0.87 &  \underline{0.93}   \\
\hline 
\end{tabular}
}
\label{UTarea}
\vspace{-5pt}
\end{table}

Additional results that measure the effect of turning off different parts of our system can be found in the supplemental material.

\vspace{0.05in} \noindent\textbf{Caltech Resident-Intruder Mouse Dataset.} As an application to agents other than humans, we also evaluate our approach in the mouse dataset of~\cite{CRIM13}. This dataset consists of long videos of interactions between pairs of mice, and since there are no by-standers in this scenario (\ie $M=N$) we assess detection, recognition and temporal localization performance only. We use the training and testing splits defined in~\cite{CRIM13}, and we use their suggested individual and pairwise descriptors: spatio-temporal interest points (STIP) that encode agent appearance,\footnote{Different from \cite{CRIM13}, we only compute STIP-based features inside the bounding boxes that enclose the mice.} and trajectory-based features (position, velocities, etc.)for individual agents, and relative trajectory features for pairwise descriptors. (See supplemental material for details.)

To classify an temporal interval in a test video that is successfully matched to one or more exemplars, we simply read the label of the top-scoring exemplar. Table \ref{CRIMAccu} shows the results using the error metric (frame-wise accuracy) defined in \cite{CRIM13}. As done in the original work, we separately measure performance using trajectories features only, STIP features only, and both. It is clear that motion trajectory information is much more important than local STIP-based features, which is not surprising given the limited articulation of the agents.

\begin{table}[h]
\vspace{-5pt}
\centering \caption{Accuracies for the proposed method and the baselines on Caltech Resident-Intruder Mouse Dataset. (See additional results in the supplementary material.)}
\footnotesize{
\begin{tabular}{|c||c|c|c|}
\hline  \% & Trajectory & STIP & Both \\
\hline \cite{CRIM13} w/o. context & 52.3 & 29.3 & 53.1\\
\hline \cite{CRIM13} w. context & 58.3 & \underline{43.0} & 61.2\\
\hline Ours w/o. ML & 49.4 & 18.8 & 50.9 \\
\hline Ours w. ML & \underline{66.0} & 31.7 & \underline{62.9}\\
\hline 
\end{tabular}
}
\label{CRIMAccu}
\vspace{-5pt}
\end{table}

Additional analysis, including different approaches to encoding trajectory information and detection accuracy as a function of interaction length can be found in the supplemental material.

\vspace{0.05in}

\noindent\textbf{Conclusion.} We have proposed an approach to find small-group interactions in space and time among a larger social clutter. We achieve simultaneous participant identification, temporal localization, and classification. These functionalities are enabled through an effective and efficient integration of multiple modules. Our approach is flexible and generic to various of applications provided that an individual behavior descriptor and a pairwise behavior descriptor are properly given. On the other hand, a further improvement of the overall performance will also rely on the robustness and discrimination of these descriptors, especially when videos are of limited quality such as the classroom videos we use. 



%The dataset consists of 20 one-minute videos of continuous executions of 6 classes of two-person interactions: shaking-hands, pointing, hugging, pushing, kicking, and punching. Each video contains at least one instance of every interaction class, where distinct activities may occur at the same time. 10 videos are taken on a parking lot and the other 10 videos are captured in a natural setting by a moving camera. As the UT-Interaction dataset presents simultaneous performance of several activities, activities that may begin and end at arbitrary times, and the presence of people who are not involved in the activity, etc., it is well fitted into the scenario considered here. 

%Since \cite{Ryoo:group} does not apply human detection and tracking and \cite{Amer:group} does not explicitly operate on the detected bounding boxes, our performance should be regarded as an overall effect from both human detection/tracking and our approach.