\noindent\textbf{UT-Interaction Dataset.} To compare with the state-of-art, we implement our approach on UT-Interaction dataset initially used in \cite{Ryoo:group} (where the reader can find the description about the dataset) and recently again in \cite{Amer:group}. We follow exactly the same setup as in \cite{Ryoo:group,Amer:group}: 20\% of all available manual segmentations, each occupied by a unique activity instance, are used as `database exemplars' for training. The remaining full (unsegmented) sequences are used for testing. In training, a 4-level pyramid is set up at each ground-truth time interval, and a temporal unit is set at half length of the bottom-level cell. A 32-dimensional histogram of the spatio-temporal features developed by \cite{Dollar:STIP} in each unit and each bounding box is calculated first against a 500-word vocabulary from K-means clustering and then by PCA, and serves as the individual activity descriptor. Another 32-dimensional histogram of the optical flow computed from OpenCV in each unit and each box is calculated against 8 directions and 4 magnitudes, and the difference between the two histograms (relative motion) serves as the pairwise descriptor between the two humans. In testing, we employ the human detector \cite{Pedro:detect}, and associate the detected boxes across frames to form continuous tracks of humans. 

\begin{table}[ht]
\centering \caption{Classification accuracies and false positive (FP) rates for the proposed method and the baselines on UT-Interaction dataset.}
\footnotesize{
\begin{tabular}{|c||c|c|}
\hline   & Accuracy (\cite{Ryoo:group}, \cite{Amer:group}, ours) & FP Rate (\cite{Ryoo:group}, \cite{Amer:group}, ours) \\
\hline Hug &  (0.875, 0.904, \underline{1.00}) & (0.075, 0.055, \underline{0.00}) \\
\hline Kick &  (0.750, 0.775, \underline{0.875})  & (0.138, 0.108, \underline{0.063})\\
\hline Point & (0.625, 0.663,  \underline{0.750}) & (\underline{0.025}, \underline{0.025}, 0.088)\\
\hline Punch & (0.500, 0.632, \underline{0.750})  & (0.201, 0.154,  \underline{0.138})\\
\hline Push & (0.750, \underline{0.782} , 0.750)  & (0.125, \underline{0.101},  0.138)\\
\hline Shake Hands &  (0.750, 0.789, \underline{1.00}) & (0.088, 0.060, \underline{0.00}) \\
\hline\hline Average &  (0.708, 0.758, \underline{0.854})  & (0.108, 0.083,  \underline{0.071})\\
\hline 
\end{tabular}
}
\label{UTaccuFP}
\end{table}


The first interesting investigation is again about the effectiveness of combining both individual descriptor and pairwise descriptor, as well as metric learning for group dissimilarity function. We implement our approach with either but not both descriptor, and without learning the dissimilarity. The performance comparison is show in Table ~\ref{UTaccuFPdegrade} (supplementary material). It is interesting to see the pairwise descriptor plays a more crucial role for this dataset: A significant performance drop arises when we only consider individual action descriptors. The next comparison is against the state-of-art methods \cite{Ryoo:group,Amer:group}. The recognition accuracies and false-alarm rates are shown in Table ~\ref{UTaccuFP}. In this evaluation, we only allow one database exemplar to produce a single response in an input, and claim a true positive only when both the class-label and the identified participants are simultaneously correct, otherwise a false positive is claimed for the exemplar class. We achieve improved accuracy and competitive false positive rate against the baselines. In the third evaluation, we specifically look into the temporal localization and participant identification performance. For temporal localization, we follow exactly the same criterion as in \cite{Amer:group}, requiring a true-positive to achieve both correct classification and a $>50\%$ ratio of the intersection to the union of the estimated interval and ground-truth. we achieve a slightly smaller area under ROC curve than the two baselines, as shown in Table ~\ref{UTarea}. Note that the temporal boundary is essentially ambiguous for these consecutively executed activities. For participant identification, we enforce a stricter criterion to require $100\%$ correct identification (in contrast to $50\%$ in \cite{Amer:group}) for a true positive, and we outperform \cite{Amer:group}. 

\begin{table}[ht]
\centering \caption{Area under ROC curve for the proposed method and the baselines on UT-Interaction dataset.}
\footnotesize{
\begin{tabular}{|c|c|c|c|}
\hline   & \cite{Ryoo:group} &  \cite{Amer:group}  &   ours \\
\hline Temporal Localization &  0.91 & \underline{0.94} &  0.89\\
\hline Participants Identification &  N/A & 0.87 &  \underline{0.93}   \\
\hline 
\end{tabular}
}
\label{UTarea}
\end{table}

\vspace{0.05in}

\noindent\textbf{Caltech Resident-Intruder Mouse Dataset.} We also tested the approach on Caltech Resident-Intruder Mouse Dataset \cite{CRIM13}, to demonstrate that our approach can be directly used for a traditional task of temporal segmentation and classification without any changes. The detailed experimental setting and results are presented in the supplementary material.


\vspace{0.05in}

\noindent\textbf{Conclusion.} We have proposed an approach to find small-group interactions in space and time among a larger social clutter. We achieve simultaneous participant identification, temporal localization, and classification. These functionalities are enabled through an effective and efficient integration of multiple modules. Our approach is flexible and generic to various of applications provided that an individual behavior descriptor and a pairwise behavior descriptor are properly given. On the other hand, a further improvement of the overall performance will also rely on the robustness and discrimination of these descriptors, especially when videos are of limited quality such as the classroom videos we use. 



%The dataset consists of 20 one-minute videos of continuous executions of 6 classes of two-person interactions: shaking-hands, pointing, hugging, pushing, kicking, and punching. Each video contains at least one instance of every interaction class, where distinct activities may occur at the same time. 10 videos are taken on a parking lot and the other 10 videos are captured in a natural setting by a moving camera. As the UT-Interaction dataset presents simultaneous performance of several activities, activities that may begin and end at arbitrary times, and the presence of people who are not involved in the activity, etc., it is well fitted into the scenario considered here. 

%Since \cite{Ryoo:group} does not apply human detection and tracking and \cite{Amer:group} does not explicitly operate on the detected bounding boxes, our performance should be regarded as an overall effect from both human detection/tracking and our approach.