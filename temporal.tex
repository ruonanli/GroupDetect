\subsection{Branch-and-Bound Temporal Localization}
\label{BB}
\vspace{-5pt}
Our third step is to determine the starting time $T_s$ and ending time $T_e$ ($1\le T_s<T_e\le T$) of the distinctive interaction. For this purpose, after the participants are determined through the best matching $W^{*}$, we recompute the dissimilarities under this specific matching $\hat{D}(\mathcal{Q}_{t}, \mathcal{D}_{s}, W^{*})$, which tells how similar the interaction of the individuals selected by $W^{*}$ at time $t$ is to the exemplar at time $s$, for all $(t,s)$ pairs. We can further get  $D^{*}(t)=\min_{s}\hat{D}(\mathcal{Q}_{t}, \mathcal{D}_{s}, W^{*})$, the minimal dissimilarity of the input interaction by the selected participants at time $t$ to the entire exemplar, as well as $s^{*}(t)=\arg\min_{s}\hat{D}(\mathcal{Q}_{t}, \mathcal{D}_{s}, W^{*})$, the time in the exemplar at which the input at time $t$ exhibits this maximum similarity.  

If in the input between starting time $t_{s}$ and ending time $t_{e}$ ($1\leq t_{s}<t_{e}\leq T$) the selected agents perform exactly the same interaction as the exemplar (between time $1$ and $S$), what should $D^{*}(t)$ and $s^{*}(t)$ ideally look like in this interval? Obviously, 1) the dissimilarities $D^{*}(t)$'s should be small for $t_{s}\le t\le t_{e}$ and be large for other $t$'s; and 2) the optimally aligned time $s^{*}(t)$ should reside at a similar relative temporal location between $1$ and $S$ as the input time $t$ resides between $t_{s}$ and $t_{e}$.

To realize 1), we use the approach in Sec. \ref{MetLearn}, where the dissimilarities $D^{*}(t)$ are driven toward 0 in the `ground-truth' interval $[t_{s}, t_{e}]$ and toward $2$ otherwise. To formally describe the temporal aligning condition in 2), we employ a temporal pyramid with a total of $L$ levels and $2^{l}$ equal-length cells at the $l$th level to encode the temporal location of a particular time in an interval. Specifically, we define $t\in\mathcal{C}(T_{s},T_{e}, l,i)$ to mean that time $t$ belongs to the $i$th cell at the $l$th level of a $L$-level temporal pyramid on the interval $[T_{s},T_{e}]$, and $s\in\mathcal{C}(1,S, l,i)$ to mean that time $s$ belongs to the $i$th cell at the $l$th level of the pyramid on the interval $[1,S]$. Then, the quantity
\begin{equation}
\vspace{-5pt}
k(t, T_{s},T_{e}, s, 1,S)\triangleq\sum^{L-1}_{l=0}\sum^{2^{l}}_{i=1} \mathbf{1}(t\in\mathcal{C}(T_{s},T_{e}, l,i))\mathbf{1}(s\in\mathcal{C}(1,S, l,i))
\end{equation}
achieves its maximum if and only if $s$ resides at the same relative temporal location between $1$ and $S$ as $t$ resides between $T_{s}$ and $T_{e}$.

When 1) and 2) are both achieved, the function
\begin{equation}
\vspace{-5pt}
q(t)\triangleq k(t, T_{s},T_{e}, s^{*}(t), 1,S)(D^{*}(t)-1)
\label{qfunction}
\end{equation}
achieves a negative value in the desirable interval $t_{s}\le t\le t_{e}$(because $k(t, T_{s},T_{e}, s^{*}(t), 1,S)$ achieves the maximum positive number and $(D^{*}(t)-1)$ is approximately equal to $-1$), and achieves a positive value otherwise, as illustrated in the lower part of Fig. \ref{diagram}. As a result, the function
\begin{equation}
\vspace{-5pt}
\label{quality}
f(T_{s}, T_{e})\triangleq\sum^{T_{e}}_{t=T_{s}}q(t)
\end{equation}
achieves the global minimum if and only if the interval $[T_{s}, T_{e}]$ is exactly aligned to the desirable interval $[t_{s}, t_{e}]$. 

The significance of function $f$ is that it satisfied the requirement for the ``quality function"  in \cite{Lampert}, which enables an efficient branch-and-bound search to obtain the optimal solutions for $T_{s}$ and $T_{e}$, instead of a sliding window at multiple scale as required for most existing work. We detail the branch-and-bound algorithm and justify its rationale in the Appendix as supplementary material.


The procedure described in Sections \ref{vote} and \ref{BB} is repeated multiple times for each input video and each exemplar. After we find a best solution of $(W^{*}, T_{s}, T_{e})$, we remove the corresponding tracks in the corresponding interval before we re-execute the same procedure for the second-best solution, and so on. In this way, we obtain multiple responses in the input for each exemplar, and  the input is also matched against multiple database exemplars. In the end, we have for the input video a pool of space-time localizations, with each of these ``detected interactions" associated through similarity scores to one or several exemplars. To classify a detected interaction, we simply apply the majority of the category labels of the top-ranked associated exemplars.

