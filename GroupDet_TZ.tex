\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{cvpr}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage[lined,algonl,boxed]{algorithm2e}
\usepackage{algorithmic}

% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[pagebackref=false,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}


%\cvprfinalcopy % *** Uncomment this line for the final submission

\def\cvprPaperID{****} % *** Enter the CVPR Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
\ifcvprfinal\pagestyle{empty}\fi

\begin{document}

%%%%%%%%% TITLE
\title{Finding Interactions in Social Clutter}
\author{ Ruonan Li and Todd Zickler\\
Harvard School of Engineering and Applied Science\\
{\tt\small \{ruonanli,zickler\}@seas.harvard.edu}
}

\maketitle
% \thispagestyle{empty}

\begin{abstract}
We consider the problem of finding distinctive group interactions in a video of a large social gathering. Given a pre-defined gallery of short exemplar interaction videos, and an input video of a large gathering where the actors are approximately tracked, we identify within the gathering small sub-groups that exhibit social interactions resembling those in the exemplars. The participants of each detected interaction are localized in space; the extent of their group interaction is localized in time; and when the gallery of exemplars is annotated with group-interaction categories, each detected interaction is  classified into one of the pre-defined categories. Our approach represents group behaviors by dichotomous collections of descriptors for individual actions and pair-wise interactions, and it includes an efficient algorithm for optimally distinguishing participants from bystanders in every temporal frame. Participant information is accumulated over time through a voting scheme that provides insensitivity to tracking errors (e.g., broken tracks, false detections) as well as variations in action rates. Most importantly, the method is general and can be applied whenever numerous interacting agents can be approximately tracked over time. We evaluate the approach using three different video collections, two that involve humans and one that involves mice.
\end{abstract}

\section{Introduction}

Social interactions are common, but they rarely take place in isolation. Conversations and other group interactions occur on busy streets, in crowded cafes, in large conference halls, and many other types of social gatherings. In these cases, before a computer vision system can \emph{recognize} distinctive group interactions, it must first \emph{detect} them by distinguishing between participants and by-standers, and by localizing these participants in space and time. This paper addresses this spatio-temporal detection problem for cases in which the actors in a large gathering can be approximately detected and tracked.

We approach this as a matching problem. Given an exemplar video of a group interaction that involves a small handful of people, we aim to detect and localize instances of similar interactions within a long video of a larger gathering. 
Specifically, we accept as input a video of a group of individuals, within which may occur an interesting interactive activity involving a handful of participants over an unknown period of time. Our approach localizes interactions of potential interest by explicitly identifying the participants within the group and estimating the beginning and end of their interaction. It also classifies these detected interactions into several categories based on the exemplar videos. Therefore, our  contribution is an approach that goes beyond most previous efforts \cite{Hongeng:act,Gong:act,Hakeem:act,McCowan:meeting,Ni:group,Choi:recogtrack,Intille:act,Vlad:group} which only recognize the activity of interest and assume that the activity occupies the entire length of the video and all humans appearing in the video are participants of the activity. Our appraoch also advances the activity detection problem from only considering a single individual \cite{Ke:detection,Yuan:detection,Shechtman:detection,Hu:detection,Laptev:detection,Duchenne:detection} into considering multi-people behaviors.

\begin{figure}[t]
\begin{center}
\includegraphics[width=\columnwidth]{voting.pdf}
\end{center}
\caption{Detecting and localizing interactions in social clutter. Given an exemplar video of an $N$-person social interaction, we seek to find similar interactions in a long input video of $M>N$ approximately-tracked actors. For each temporal frame in the exemplar, the $N$ best-matching participants are identified separately in each input video frame, and the matches are assigned scores. Matching scores are accumulated over time through a voting scheme that is insensitive to tracking errors and changes in action rates, and this produces a spatial localization of the $N$ participating actors. Their interaction is then localized in time using an efficient branch and bound search.}
\label{diagram}
\end{figure}

Given an input video, our approach attempts to match a subset of the observed agents to each of a collection of annotated social interaction exemplars in a database. As depicted in Figure \ref{diagram}, we provide a ranked list of annotated exemplars, with each exemplar associated with the best-matching `roster' of participating individuals in the input video, an estimate of the temporal extent of this best-matching interaction, and a companion score that indicates the similarity between this best-matching interaction and the exemplar. Our output allows classification, for example, by inferring for each interaction detected in the input a class label based on the labels of top-ranked exemplars. An advantage of our approach is its generality: Social interactions are represented using a sequence of ensembles of per-person features to encode appearance and/or motion of each participant, along with a separate sequence of ensembles of pair-wise contextual features to encode relative appearance and/or motion of each participant with respect to all others. The overall approach includes an efficient ensemble matching mechanism to compare the set of descriptors from an exemplar and the set of descriptors from the input, and learns the optimal matching parameters discriminatively. These two efforts enable both a Hough voting with dual accumulator arrays to robustly distinguish the participants from all and a Branch-and-Bound search for temporal localization. Therefore, it can be applied in most scenarios that involve agents interacting within a larger group: We evaluate the approach using three very different datasets. One is the UT-Interaction Dataset~\cite{Ryoo:group}, one is a new collection of videos from an `interactive classroom' (e.g.~\cite{Crouch:PI}), where students self-organize into small groups and engage in discussion, and the other is the Caltech Resident-Intruder Mouse dataset \cite{CRIM13} by which we show that our approach is applicable to non-human scenario and directly accommodates traditional tasks without changes.

Our work builds on a small collection of related work: \cite{Li:segmentation} identifies the relevant motion from a clutter, \cite{Cristani:discovery} detects pre-defined geometric configuration of individual poses, and \cite{Lan:retrieval} retrieves individual actions in the context of surrounding humans. Those more similar to ours are \cite{Ryoo:group,Amer:group}: The former addresses recognition and time-localization of interactions, and the latter implicitly infers participants from a generative model for a set of histograms of pose-coded detections computed at a dense spatio-temporal grid. More discussion about the relationship between these and our work is deferred until Section \ref{MetLearn}.

\section{Matching and Localizing Social Interactions}


Consider an input video consisting of $T$ temporal units, with each unit being a frame or several consecutive frames. By applying domain-appropriate detection and tracking, assume we obtain $M$ space-time tracks of bounding boxes enclosing $M$ faces or bodies. Note that these $M$ tracks may include non-participants and false-alarm bounding boxes resulted from an imperfect detector. The $M$ targets are to be compared with the social interaction exemplars pre-stored in a database. Such an exemplar consists of $S$ temporal units and $N$ targets that are correctly detected and tracked and all participating in an coherent social interaction. As the input may include non-participants and false-positive tracks, it contains the same or more targets than the exemplar, \textit{i.e.}, $N\leq M$.

The first question is how to represent the two sets of size $M\times T$ and $N\times S$. A social interaction is not only a collection of individual activities, but is more crucially characterized by the mutual contexts between individuals. Our representation for the input tracks is consequently made up of two parts. The first part is a collection of $M\times T$ $d_{I}$-dimensional descriptors $\{\mathbf{f}_{m,t}\}_{m=1,2,\cdots,M, t=1,2,\cdots,T}$, where $\mathbf{f}_{m,t}$ encodes the individual activity of the $m$th target at time $t$. The other part is a collection of $M\times (M-1)\times T$ $d_{P}$-dimensional pairwise contextual descriptors $\{\mathbf{g}_{m,m',t}\}_{m,m'=1,2,\cdots,M, m\neq m', t=1,2,\cdots,T}$, where $\mathbf{g}_{m,m',t}$ encodes dynamic visual properties of target $m$ that are exhibited relative to those of target $m'$ at time $t$. Loosely speaking, $\mathbf{g}_{m,m',t}$ describes the `influence' that target $m'$ exhibits over target $m$ at time $t$, with the possibility that $\mathbf{g}_{m,m',t}\ne\mathbf{g}_{m',m,t}$. The form of descriptors $\mathbf{f}_{m,t}$ and $\mathbf{g}_{m,m',t}$ will be application dependent, and will be further discussed in experiment section. For each exemplar, we have two similar descriptor collections $\{\mathbf{f}^{D}_{n,s}\}_{n=1,2,\cdots,N, s=1, 2,\cdots, S}$ and $\{\mathbf{g}^{D}_{n,n',s}\}_{n,n'=1,2,\cdots,N, n\neq n', s=1, 2,\cdots, S}$. We denote the descriptor ensemble for the input at time $t$ to be $\mathcal{Q}_{t}\triangleq\{\mathbf{f}_{m,t},\mathbf{g}_{m,m',t}\}$, and that for the exemplar at time $s$ as $\mathcal{D}_{s}\triangleq\{\mathbf{f}^{D}_{n,s},\mathbf{g}^{D}_{n,n',s}\} $. As a result, the input can be represented as $\mathcal{Q}\triangleq\{\mathcal{Q}_{t}\}_{1\leq t\leq T}$ and the exemplar $\mathcal{D}\triangleq\{\mathcal{D}_{s}\}_{1\leq s\leq S}$.


Given the input and the exemplar, our primary tasks are to identify from the $M$ targets in the input $N$ targets that demonstrate the most similar socially interactive pattern as the $N$ individuals in the exemplar, as well as to temporally localize the extent of the interaction within the interval $[1, T]$. To formally describe the former task, consider the fact that if one of the $M$ targets is regarded as a participant in an interaction as exemplified by the exemplar $\mathcal{D}$, its behavior should properly match to the behavior of one of the $N$ individuals in the exemplar $\mathcal{D}$. Therefore, we use a $N\times M$ binary matrix $W=[w_{nm}]\in\{0,1\}^{N\times M}$ to formally represent the participant identification, where $w_{nm}=1$ means that the $n$th exemplar individual is matched to the $m$th input target and $w_{nm}=0$ means unmatched targets. For unambiguous matching, we expect each individual in the exemplar to find its unique partner target in the input. This implies that $\sum_{m}w_{nm}=1, \forall n$ and $\sum_{n}w_{nm}\leq 1, \forall m$, \textit{i.e.}, $W\mathbf{1}=\mathbf{1}$ and $\mathbf{1}^{T}W\leq\mathbf{1}^{T}$\footnote{Note that false-alarm tracks are already accounted for in the partial matching, and if we set the values of the descriptors of an out-of-scene target to be sufficiently large (or small) numbers, this target will not be matched with any target in the exemplar.}. Denote $\mathcal{W}\triangleq\{W\in\{0,1\}^{N\times M}| W\mathbf{1}=\mathbf{1}, \mathbf{1}^{T}W\leq\mathbf{1}^{T}\}$, and our former task is essentially to find $W^{*}\in\mathcal{W}$, which encodes the best matching between the input $\mathcal{Q}$ and the exemplar $\mathcal{D}$. To formally describe the latter task is straightforward: We simply look for the starting time $T_{s}$ and ending time $T_{e}$, $1\le T_{s}<T_{e}\le T$, such that the interactive pattern of input $\mathcal{Q}$ during $[T_{s}, T_{e}]$ demonstrate the best similarity with the exemplar $\mathcal{D}$.

Our computational framework has three steps as illustrated in Figure \ref{diagram}. The first step is to evaluate $T\times S$ dis-similarity scores $D(\mathcal{Q}_{t}, \mathcal{D}_{s})$ together with their 'instantaneous optimal matching' $W^{t,s}$ between each time unit  $1\le t\le T$ and each time unit  $1\le s\le S$ (Section \ref{agg}). This is followed by a dual-accumulator Hough voting procedure to find the best matching $W^{*}$ (Section \ref{vote}). Finally, an efficient branch-and-bound search estimates the temporal extent $[T_{s}, T_{e}]$ (Section \ref{BB}).  


\subsection{Dual-Accumulator Hough Voting}
\label{vote}

Before introducing the approach to compute the instantaneous optimal matching, we first discuss the dual-accumulator Hough voting procedure by which we identify the participants, \textit{i.e.}, compute the best matching matrix $W^{*}$. Note that  $W^{*}$ specifies $N$ targets which demonstrate a consistently similar social interactive pattern as the exemplar $\mathcal{D}$ over a substantial period in the input $\mathcal{Q}$. This means that if we `zoom-in' to a temporal unit in this period, say $\mathcal{Q}_{t}$ from the incoming video, and `zoom-in' to a temporal unit, say $\mathcal{D}_{s}$, from the exemplar, the matrix $W^{*}$ should also give the best matching between the two `instantaneous' interactions $\mathcal{Q}_{t}$ and $\mathcal{D}_{s}$. As a consequence, if we compute the optimal matching between the instantaneous interaction $\mathcal{Q}_{t}, \forall t$ and the instantaneous interaction $\mathcal{D}_{s}, \forall s$, the same matching $W^{*}$ will emerge many times. In other words, the desired $W^{*}$ will be supported by a large number of  optimal matchings between all instantaneous interactions. 

Formally, we denote the optimal matching matrix between instantaneous interactions $\mathcal{Q}_{t}$ and $\mathcal{D}_{s}$ as $W^{t,s}$, and compute a dis-similarity score $D(\mathcal{Q}_{t}, \mathcal{D}_{s})$ which yields a smaller number if the optimal $N$ targets in $\mathcal{Q}_{t}$ better matches with $\mathcal{D}_{s}$ and a larger number otherwise. (The approach to compute $W^{t,s}$ and $D(\mathcal{Q}_{t}, \mathcal{D}_{s})$ is introduced in Section \ref{agg}.) The scenario we face now resembles the settings for a general Hough Transform, where the desired line/shape parameters are supported by the maximum number of image feature observations. In a general Hough Transform, a parameter set and an observation set are established, for every observation one enumerates over all parameters and determines every parameter that is compatible with the observation, the count for that compatible parameter in an accumulator array for all parameters is increased by 1, and finally the parameter receiving the maximum number of counts is reported. Here, we regard the parameter set as consisted of two parts - the set of all possible matchings $\mathcal{W}$ and the exemplar $\mathcal{D}$, and we regard the observation set as the input $\mathcal{Q}$. However, a fact to be taken into account is that the overall optimal matching may not simply be achieved on the matching receiving the maximum number of counts, but on the matching which yields the maximum similarity with the exemplar. Therefore, in addition to maintaining an accumulator array for the counts on all matchings $\mathcal{W}$, we also maintain a companion accumulator array for the corresponding dissimilarity measures achieved on them. 

\begin{figure}[t]
\begin{center}
\includegraphics[scale=1.2]{all_illu.png}
\end{center}
\caption{(a) The temporal neighborhood used in to compute \ref{softvote}. See Section \ref{vote} for details. (b) Illustration of the pairwise contextual descriptors for groups comprised of three or more participants in the classroom interaction database. See Section \ref{expall} for details. (c) Similar illustrations for groups involving only two participants. See Section \ref{expall} for details.}
\label{all_illu}
\end{figure}

Instead of directly using the dissimilarity scores $D(\mathcal{Q}_{t}, \mathcal{D}_{s})$ for the companion array, another fact regarding temporal consistency is that if the matching $W^{t,s}$ is optimal at time pair $(t,s)$, the optimal matchings at the temporally nearby (input, exemplar) time pairs should be also achieved by the same matrix $W^{t,s}$. This means that the matching matrices of these nearby times should all agree, and their dissimilarities $D$'s should all be small.  As a result, we use the follow temporal-consistency preserved dissimilarity measure to vote in the companion array for $W^{t,s}$
\begin{equation}
\label{softvote}
v(W^{t,s})=\sum_{(t',s')\in\mathcal{N}(t,s)}(\|W^{t,s}-W^{t',s'}\|_{1}+1)D(\mathcal{Q}_{t'}, \mathcal{D}_{s'}).
\end{equation}
Here $\mathcal{N}(t,s)$ is a temporal neighborhood of $(t,s)$ in which we enforce the consistency and it is depicted in Figure \ref{all_illu} (a), where the pair $(t,s)$ is shown in black square and the neighborhood is shown in shaded area. Neighborhood sizes $t_{w}$ and $s_{w}$ are taken as a quarter of the length of a cell on the bottom of the pyramid (See Section \ref{BB})\footnote{When the neighborhood extends out of video boundary, we only consider the cells within the boundary and normalize the vote by the number of cells actually involved.}. (\ref{softvote}) implies that, when evaluating the vote $v(W^{t,s})$ for input time $t$ against exemplar time $s$, we also look at the interactive behavior happening ahead of (resp. after) $t$, and expect that the two aforementioned consistencies are maintained so that the the group behavior happening ahead of (resp. after) $t$ resembles that happening ahead of (resp. after) $s$ in the exemplar, in the sense of a smaller $D$ and a same matching matrix (a same group of matched participants). Eventually, a better matching will receive a lower vote. 

The dual-accumulator Hough voting procedure by which we identify the participants is summarized in Algorithm \ref{Algo:1}, where in the last two steps we find among those matching matrices which receive a substantial number of supports from instantaneous matchings the best matching $W^{*}$ with the lowest average dissimilarity to the exemplar.
\begin{algorithm}
\footnotesize{
\begin{enumerate}
\item Clear both accumulator arrays;
\item For each $t\in[1,T], s\in[1,S]$, increment the count in the cell corresponding to $W^{t,s}$ and increase the vote in the companion array corresponding to $W^{t,s}$ by $v(W^{t,s})$;
\item Identify a subarray of cells receiving more than $\frac{S}{2}$ counts, and normalize the dissimilarity votes in the companion subarray by corresponding counts;
\item Report the matching matrix $W^{*}$ to be the one in the subarray receiving the minimum normalized dissimilarity vote.
\end{enumerate}
}
\caption{\small Dual-accumulator Hough voting procedure for identify the participants (\textit{i.e.}, the best matching $W^{*}$).}
\label{Algo:1}
\end{algorithm}



\subsection{Branch-and-Bound Temporal Localization}
\label{BB}


Once the participants are determined through the best matching $W^{*}$, we recompute the dissimilarities under this specific matching $\hat{D}(\mathcal{Q}_{t}, \mathcal{D}_{s}, W^{*})$ for all $(t,s)$ pairs (See \ref{agg}), and select $D^{*}(t)=\min_{s}\hat{D}(\mathcal{Q}_{t}, \mathcal{D}_{s}, W^{*})$ as well as $s^{*}(t)=\arg\min_{s}\hat{D}(\mathcal{Q}_{t}, \mathcal{D}_{s}, W^{*})$. Essentially, $\hat{D}(\mathcal{Q}_{t}, \mathcal{D}_{s}, W^{*})$ tells how similar the activity of the individuals selected by $W^{*}$ at time $t$ is to the exemplar activity at time $s$, $D^{*}(t)$ characterizes the minimal dissimilarity of the input activity by the selected participants to the entire exemplar, and $s^{*}(t)$ indicates the time in the exemplar at which the input at time $t$ exhibits this maximum similarity.  Suppose that in the input between starting time $t_{s}$ and ending time $t_{e}$ ($1\leq t_{s}<t_{e}\leq T$) the selected group of people perform exactly the same activity as the exemplar (between time $1$ and $S$), then ideally not only the dissimilarities $D^{*}(t)$'s should be small for $t_{s}\le t\le t_{e}$ and be large for other $t$'s, but also the optimally aligned time $s^{*}(t)$ should reside at a similar relative temporal location between $1$ and $S$ as the input time $t$ resides between $t_{s}$ and $t_{e}$.

This observation motivates a design of efficient temporal localization. On the one hand, through the effort to be introduced in Section \ref{MetLearn}, the dissimilarities $D^{*}(t)$ are driven toward zero in the `ground-truth' interval $[t_{s}, t_{e}]$ , and toward a large positive number $2\Delta$ otherwise. On the other hand, we employ a temporal pyramid with a total of $L$ levels and $2^{l}$ equal-length cells at the $l$th level to encode the temporal location of a particular time during a period. Specifically, we define $t\in\mathcal{C}(T_{s},T_{e}, l,i)$ to mean that time $t$ belongs to the $i$th cell at the $l$th level of a $L$-level temporal pyramid on the interval $[T_{s},T_{e}]$, and $s\in\mathcal{C}(1,S, l,i)$ to mean that time $s$ belongs to the $i$th cell at the $l$th level of the pyramid on the interval $[1,S]$. As a result, the quantity
\begin{equation}
k(t, T_{s},T_{e}, s, 1,S)\triangleq\sum^{L-1}_{l=0}\sum^{2^{l}}_{i=1} \mathbf{1}(t\in\mathcal{C}(T_{s},T_{e}, l,i))\mathbf{1}(s\in\mathcal{C}(1,S, l,i))
\end{equation}
achieves the maximum if and only if $s$ resides at the same relative temporal location between $1$ and $S$ as $t$ resides between $T_{s}$ and $T_{e}$.

Taking both considerations above into account, apparently the function
\begin{equation}
\label{quality}
f(T_{s}, T_{e})\triangleq\sum^{T_{e}}_{t=T_{s}}k(t, T_{s},T_{e}, s^{*}(t), 1,S)(D^{*}(t)-\Delta)
\end{equation}
achieves the global minimum if and only if the interval $[T_{s}, T_{e}]$ is exactly aligned to the desirable interval $[t_{s}, t_{e}]$. (Note that in this case all terms $k(t, T_{s},T_{e}, s^{*}(t), 1,S)$ achieves the maximum positive number, and all terms $(D^{*}(t)-\Delta)$ are approximately equal to $-\Delta$.)


It is straightforward to verify that the above designed function $f$ has satisfied the requirement for the `quality function' studied in \cite{Lampert}, which enables the efficient branch-and-bound search to obtain the optimal solutions for $T_{s}$ and $T_{e}$, instead of a sliding window at multiple scale as adopted by the majority of existing work. To apply branch-and-bound algorithm to minimize \ref{quality}, we first specify the spaces where $T_{s}$ and $T_{e}$ may take a value. We denote the length of the shortest exemplar activity as $T_{min}$, then we assume $1\leq T_{s}\leq T-T_{min}+1$ and $T_{min}+1\leq T_{e}\leq T$. Additional constraint may be imposed, such as $T_{min}\leq T_{e}-T_{s}$. Given these information,  the temporal branch-and-bound algorithm, as a companion to the 2-D case studied in \cite{Lampert}, can be derived as in Algorithm \ref{Algo:2}.  In this algorithm, $\hat{f}(T_{s,low}, T_{s,upp}, T_{e,low}, T_{e,upp})$ is a lower bound of the values of the quality function evaluated on all intervals enclosed in $[T_{s,low}, T_{s,upp}]\times [T_{e,low}, T_{e,upp}]$. To calculate this lower bound, we define
\begin{equation}
\begin{split}
&\hat{f}(T_{s,low}, T_{s,upp}, T_{e,low}, T_{e,upp})\\
&=\sum^{L-1}_{l=0}\sum^{2^{l}}_{i=1} \hat{f}(T^{l,i}_{s,low}, T^{l,i}_{s,upp}, T^{l,i}_{e,low}, T^{l,i}_{e,upp})
\end{split}
\end{equation}
where $T^{i,l}_{s}, T^{i,l}_{e}$ are the boundaries of cell $\mathcal{C}(T_{s},T_{e}, l,i)$. In other words, we use the summation of the lower bounds of all cells in the pyramid as the lower bound of the entire interval. The evaluation of $\hat{f}(T^{l,i}_{s,low}, T^{l,i}_{s,upp}, T^{l,i}_{e,low}, T^{l,i}_{e,upp})$, however, is a $\mathcal{O}(1)$ operation with the help of integral dissimilarities $I(t)$ of those negative group dissimilarities $D^{*}(t)$ over $t$. Specifically, let
\begin{equation}
I(t)=\sum^{t}_{t'=1} \min(0,D^{*}(t))
\end{equation}
which only needs to be computed once. Then the lower bound for the cell $\mathcal{C}(T_{s},T_{e}, l,i)$ can be obtained as
\begin{equation}
\hat{f}(T^{l,i}_{s,low}, T^{l,i}_{s,upp}, T^{l,i}_{e,low}, T^{l,i}_{e,upp})=I(T^{l,i}_{e,upp})-I(T^{l,i}_{s,low}).
\end{equation}


\begin{algorithm*}[t]
\begin{enumerate}
\footnotesize{
\item Initialize: Let $T_{s,low}=1$, $T_{s,upp}=T-T_{min}+1$, $T_{e,low}=T_{min}+1$, and $T_{e,upp}=T$; Initialize priority queue $Q$ as empty; 
\item Do
\begin{itemize}
\item If $T_{s,upp}-T_{s,low} \ge T_{e,upp}-T_{e,low}$\\
$T^{(1)}_{s,low}\leftarrow T_{s,low}$, $T^{(1)}_{s,upp}\leftarrow T_{s,low}+\frac{T_{s,upp}-T_{s,low}}{2}$,
$ T^{(1)}_{e,low}\leftarrow T_{e,low}$, $T^{(1)}_{e,upp}\leftarrow T_{e,upp}$,
$T^{(2)}_{s,low}\leftarrow T_{s,low}+\frac{T_{s,upp}-T_{s,low}}{2}$, $T^{(2)}_{s,upp}\leftarrow T_{s,upp}$,
$ T^{(2)}_{e,low}\leftarrow T_{e,low}$, $T^{(2)}_{e,upp}\leftarrow T_{e,upp}$;\\
else \\
$T^{(1)}_{s,low}\leftarrow T_{s,low}$, $T^{(1)}_{s,upp}\leftarrow T_{s,upp}$,
$ T^{(1)}_{e,low}\leftarrow T_{e,low}$, $T^{(1)}_{e,upp}\leftarrow T_{e,low}+\frac{T_{e,upp}-T_{e,low}}{2}$,
$T^{(2)}_{s,low}\leftarrow T_{s,low}$, $T^{(2)}_{s,upp}\leftarrow T_{s,upp}$,
$ T^{(2)}_{e,low}\leftarrow T_{e,low}+\frac{T_{e,upp}-T_{e,low}}{2}$, $T^{(2)}_{e,upp}\leftarrow T_{e,upp}$;
\item If $T_{min}\leq T^{(1)}_{e,upp}-T^{(1)}_{s,low}$,
push $(T^{(1)}_{s,low}, T^{(1)}_{s,upp}, T^{(1)}_{e,low}, T^{(1)}_{e,upp},\hat{f}(T^{(1)}_{s,low}, T^{(1)}_{s,upp}, T^{(1)}_{e,low}, T^{(1)}_{e,upp}))$ into $Q$;\\
\item If $T_{min}\leq T^{(2)}_{e,upp}-T^{(2)}_{s,low}$,
push $(T^{(2)}_{s,low}, T^{(2)}_{s,upp}, T^{(2)}_{e,low}, T^{(2)}_{e,upp},\hat{f}(T^{(2)}_{s,low}, T^{(2)}_{s,upp}, T^{(2)}_{e,low}, T^{(2)}_{e,upp}))$ into $Q$;\\
\item Let $(T_{s,low}, T_{s,upp}, T_{e,low}, T_{e,upp})$ be the tuple in $Q$ achieving the minimal $\hat{f}$;
\end{itemize}
Until $T_{s,low}=T_{s,upp}, T_{e,low}=T_{e,upp}$.
\item Output: $T_{s}\leftarrow T_{s,low}, T_{e}\leftarrow T_{e,low}$. 
}
\end{enumerate}
\caption{\small Branch-and-bound search for temporal localization.}
\label{Algo:2}
\end{algorithm*}

The procedure described in Sections \ref{vote} and \ref{BB} is repeated multiple times for each input video and each exemplar. After we find a best solution of $(W^{*}, T_{s}, T_{e})$, we can remove the corresponding tracks in the corresponding interval before we re-execute the same procedure for the second-best solution, and so on. In this way, we obtain multiple responses in the input for each exemplar, and  the input is also matched against multiple database exemplars. In the end, we have for the input video a pool of space-time localizations, with each of these `detected social interactions' associated through similarity scores to one or several exemplars. To classify a a detected social interaction into a category, we simply apply majority voting using the category labels of the highly-ranked exemplars associated with that social interaction.


\subsection{Instantaneous Optimal Matching}
\label{agg}

We are now in a position to discuss the first step in our framework, which serves dissimilarity $\hat{D}(\mathcal{Q}_{t}, \mathcal{D}_{s}, W^{*})$ to the third step, and the score $D(\mathcal{Q}_{t}, \mathcal{D}_{s})$ as well as instantaneous optimal matching $W^{t,s}$ to the second step. With the instantaneous ensembles $\mathcal{Q}_{t}=\{\mathbf{f}_{m,t},\mathbf{g}_{m,m',t}\}$ and $\mathcal{D}_{s}=\{\mathbf{f}^{D}_{n,s},\mathbf{g}^{D}_{n,n',s}\}$, we seek to compute a dissimilarity between them. While the distance between two point ensembles can be computed using standard methods such as Hausdorff distance, dissimilarity between our descriptor ensembles must account for the facts that the ensembles contain structural information between individuals (contextual descriptors), and are `noisy' in the sense of including non-participant tracks and false targets. Moreover, the $M$ input targets can only be partially matched with the $N$ exemplar targets. 

While we will not define them until Section \ref{MetLearn}, suppose for now that there exists an `atomic' distance function $d_{I}(\mathbf{f}_{m,t}, \mathbf{f}^{D}_{n,s}) $ for comparing two individual descriptors $\mathbf{f}_{m,t}$ and $\mathbf{f}^{D}_{n,s}$, as well as another atomic function $d_{P}(\mathbf{g}_{m,m',t}, \mathbf{g}^{D}_{n,n',s}) $ for comparing two contextual descriptors $\mathbf{g}_{m,m',t}$ and $\mathbf{g}^{D}_{n,n',s}$. What we require to compute is an overall dissimilarity $\hat{D}(\mathcal{Q}_{t}, \mathcal{D}_{s}, W^{*})$ or $D(\mathcal{Q}_{t}, \mathcal{D}_{s})$ aggregated from the atomic distances. Recall that the  matching matrix $W\in\mathcal{W}$ encodes which of the input targets is mapped to each of the exemplar agents, and therefore it provides a natural aggregation mechanism as 
\begin{equation}
\begin{split}
&\hat{D}(\mathcal{Q}_{t}, \mathcal{D}_{s}, W)=\\
&\sum_{nm}w_{nm}d_{I}(\mathbf{f}_{m}, \mathbf{f}^{D}_{n})+\sum_{nmn'm'}w_{nm}w_{n'm'}d_{P}(\mathbf{g}_{m,m',s}, \mathbf{g}^{D}_{n,n',t}),
\end{split}
\end{equation}
in which the dissimilarity of the two ensembles under a particular matching $W$ is computed by adding all atomic distances between associated pairs.

Consequently, the score $D(\mathcal{Q}_{t}, \mathcal{D}_{s})$ becomes naturally the one from the optimal matching, \textit{i.e.}, the minimum overall dissimilarities $\hat{D}$'s over all possible matching matrices: $D(\mathcal{Q}_{t}, \mathcal{D}_{s})=\min_{W}\hat{D}(\mathcal{Q}_{t}, \mathcal{D}_{s}, W)$. At the same time, $W^{t,s}=\arg\min_{W}\hat{D}(\mathcal{Q}_{t}, \mathcal{D}_{s}, W)$ essentially identifies the participants among the input targets which share the maximum similarity with the database exemplar. 
 
 In sum, we compute the group dissimilarity $D(\mathcal{Q}_{t}, \mathcal{D}_{s})$ by solving the optimization problem
\begin{equation}
\label{Prob1}
\min_{W}\hat{D}(\mathcal{Q}_{t}, \mathcal{D}_{s}, W), \textup{s.t.} W\in\mathcal{W}.
\end{equation}
Let $\mathbf{w}$ be a long vector by stacking the columns of $W^{t,s}$, the objective in (\ref{Prob1}) can be also expressed as $\min_{\mathbf{w}}\mathbf{c}^{T}\mathbf{w}+\mathbf{w}^{T}H\mathbf{w}$, in which $\mathbf{c}$ is a $MN\times 1$ vector whose elements come from corresponding individual descriptors $d_{I}(\mathbf{f}_{m,t}, \mathbf{f}^{D}_{n,s})$'s, and $H$ is a $MN\times MN$ matrix whose elements are the corresponding contextual descriptors $d_{P}(\mathbf{g}_{m,m',t}, \mathbf{g}^{D}_{n,n',s})$'s. Though this optimization problem has integer constraints and is generally not convex or concave as $H$ is generally not positive or negative definite, a continuous quadratic concave programming without integer constraints can be proved to be equivalent to (\ref{Prob1}) and thus (\ref{Prob1}) can be efficiently solved. We introduce and justify the equivalent optimization in \ref{proof} and solve it using the CVX toolbox \cite{cvx}.

\subsubsection{Equivalent Optimization for Problem (\ref{Prob1})}
\label{proof}

We rewrite (\ref{Prob1}) as
\begin{equation}
\begin{split}
&\min_{\mathbf{w}}\mathbf{c}^{T}\mathbf{w}+\mathbf{w}^{T}H\mathbf{w}\\
&\textup{s.t.} W\in\{0,1\}^{N\times M}, W\mathbf{1}=\mathbf{1}, \mathbf{1}^{T}W\leq\mathbf{1}^{T}.
\end{split}
\end{equation}
Instead of directly tackling it, we solve the following optimization.
\begin{equation}
\label{Prob2}
\begin{split}
&\min_{\mathbf{w}}\mathbf{c}^{T}\mathbf{w}+\mathbf{w}^{T}H\mathbf{w}+\hat{\mathbf{c}}^{T}\mathbf{w}+\mathbf{w}^{T}\hat{H}\mathbf{w}\\
&\textup{s.t.}  W \in[0,1]^{N\times M}, W\mathbf{1}=\mathbf{1}, \mathbf{1}^{T}W\leq\mathbf{1}^{T},
\end{split}
\end{equation}
where $\hat{\mathbf{c}}=[\sigma_{1},\sigma_{2},\cdots,\sigma_{MN}]^{T}$, $\hat{H}=diag\{-\sigma_{1},-\sigma_{2},\cdots,-\sigma_{MN}\}$, and $\sigma_{i}$ is a sufficiently large number satisfying $\sigma_{i}>\sum^{MN}_{j=1,j\neq i}|H_{ij}|+H_{ii}$.

Note that $\hat{H}$ defined in this way imposes a negative strictly dominant diagonal to $H$ and the quadratic term $\hat{H}+H$ is strictly negative definite. Therefore, (\ref{Prob2}) is a concave programming in the convex unit hypercube $[0,1]^{N\times M}$ and will achieve its minimum at one of the feasible vertices. The feasible vertices, meanwhile, are exactly the feasible solutions of (\ref{Prob1}), and at these vertices, the values of the objective of (\ref{Prob2}) are equal to those of (\ref{Prob1}) due to the cancellation brought by $\hat{\mathbf{c}}$. It is therefore implied that by solving the much more efficient Problem (\ref{Prob2}) we obtain the exact solution for the original Problem (\ref{Prob1}).


\section{Large-Margin Ensemble Dissimilarity}
\label{MetLearn}


Recall that in Section \ref{vote}, we expect the score $D$ to be small when a temporal unit in the input is best aligned to a temporal unit in the exemplar by the `correct' matching matrix. In Section \ref{BB}, we expect the dissimilarity $D^{*}(t)$ are driven toward zero in the `ground-truth' interval $[t_{s}, t_{e}]$ , and toward a large positive number $2\Delta$ otherwise. For both purposes, in this section we learn an effective ensemble dissimilarity measure using the annotated database. Recall that the ensemble dissimilarity $D$ or $\hat{D}$ is aggregated from `atomic' descriptor distances $\{ d_{I}(\mathbf{f}_{m,t}, \mathbf{f}^{D}_{n,s}), d_{P}(\mathbf{g}_{m,m',t}, \mathbf{g}^{D}_{n,n',s})\}$, and the ensemble dissimilarity learning therefore narrows down to learning effective `atomic'  distances.  We parameterize an atomic distance function using a Mahalanobis distance metric, \textit{i.e.}, let $d_{I}(\mathbf{f}, \mathbf{f}')=(\mathbf{f}-\mathbf{f}')^{T}\Sigma_{I}(\mathbf{f}-\mathbf{f}')$ be the atomic distances between two individual activity descriptors, and $d_{P}(\mathbf{g}, \mathbf{g}')=(\mathbf{g}-\mathbf{g}')^{T}\Sigma_{P}(\mathbf{g}-\mathbf{g}')$ be that between two contextual descriptors, where $\Sigma_{I}\succeq 0$ and $\Sigma_{P}\succeq 0$  are positive semi-definite matrices.  

We learn each pair of Mahalanobis metrics $(\Sigma_{I}, \Sigma_{P})$ for each number of participants separately. To do so, we construct two different types of collections from our database exemplars. The first collection, $\mathcal{P}$, contains all pairs of instantaneous interaction ensembles that are similar to each other. The second collection, $\mathcal{M}$, is comprised of all ordered triples $(h,k,l)$ in which ensemble $h$ is similar to ensemble $k$ but they two  are dissimilar to ensemble $l$. In the present case, we build the collection $\mathcal{P}$ and the ($(h,k)$-indexed) similar pairs in collection $\mathcal{M}$ using all instantaneous ensemble pairs with their ground-truth matching matrices from the same category extracted from the same cell at the lowest level of the pyramid. Similarly, we include in the ($(h,l)$-indexed and $(k,l)$-indexed) dissimilar pairs  of collection $\mathcal{M}$ the ensembles extracted from: 1) different pyramid cells of same-category exemplars; 2) different-category exemplars in the same cell; and 3) same-category same-cell ensembles with simulated wrong matching matrices. The `background' exemplars describing non-interaction intervals are included in $\mathcal{M}$ as well. Having defined the collections $\mathcal{P}$ and $\mathcal{M}$, we find the Mahalanobis metrics by solving
\begin{equation}
\label{classify}
\begin{split}
&\min_{\Sigma_{I},\Sigma_{P}} \sum_{(u,v)\in\mathcal{P}}\hat{D}(\mathcal{D}_{u}, \mathcal{D}_{v}, W_{u,v})+\gamma\sum_{(h,k,l)\in\mathcal{M}}\xi_{h,k,l}\\
&\textup{s.t.}  \hat{D}(\mathcal{D}_{h}, \mathcal{D}_{l}, W_{h,l})-\hat{D}(\mathcal{D}_{h}, \mathcal{D}_{k}, W_{h,k})\ge 2\Delta-\xi_{h,k,l}, \\
&\xi_{h,k,l}\ge0, \Sigma_{I}\succeq 0, \Sigma_{P}\succeq 0, \forall (h,k,l)\in\mathcal{M}.
\end{split}
\end{equation}
Note that $\Delta$ is the large positive number as used in (\ref{quality}), and (\ref{classify}) will encourage a similar pair to contribute a negative summand around $-\Delta$ to (\ref{quality}), and encourage a dissimilar pair to contribute approximately $+\Delta$ summand,  thus enabling the branch-and-bound search as introduced in Section \ref{BB}.  The minimization over either $\Sigma_{I}$ or $\Sigma_{P}$ falls into the framework of large margin nearest neighbor (LMNN) formulation \cite{Weinberger:ML}. Consequently, we simply decompose problem (\ref{classify}) into independent LMNN tasks and employ \cite{Weinberger:ML} for each of them.

A recap of the entire approach presented above distinguish our work from similar ones \cite{Ryoo:group,Amer:group}: \cite{Ryoo:group} does not identify the participants, and relies on a set of pre-determined application-specific rules for temporal detection. \cite{Amer:group} builds a probabilistic generative model for a set of histograms of pose-coded detections computed at a dense spatio-temporal grid and implicitly infers participants using these `hidden' histograms, while we adopt a data-driven bottom-up strategy. Though parametric generative model such as used in \cite{Ryoo:group} is powerful in explaining human motion governed by a structured underlying dynamic, our approach is more generic and flexible in addressing either structured or less-structured dynamics such as those in our classroom video collection. We achieve quite reliable detection and tracking in our experiments, and our approach to directly match these tracks of targets turns out effective as to be demonstrated in Section \ref{expall}. As the low-level detection and tracking algorithms continue to improve, we expect our framework to become more applicable as well.


\section{Experiments}
\label{expall}


\noindent\textbf{Classroom Interaction Database.} As mentioned, we have collected and annotated a database of 100 video clips capturing students' behaviors in interactive class sessions. The students are seated but occasionally engaged into small-group discussions. The groups are formed in an ad-hoc manner and vary in geometric configurations. The 15 fps videos come from five cameras covering different areas of a lecture hall, the lengths of them vary from one minute to four minutes, and the numbers of individuals in the video varies from six to twenty. See Figure \ref{diagram} for example frames. We have applied an OpenCV face detector to the videos, and generated long tracks of the bounding boxes using a combination of OpenCV mean-shift tracking and dominant optical flows in the bounding boxes. We manually eliminate the false-alarm boxes/tracks and recover (very few) miss-detections when using the video for building ground-truth exemplars, and do not involve manual correction when testing our approach. In each video, we have manually identified the participants of all two-person, three-person, and four-person interactions as well as their exact occurrence intervals. Totally, 254 two-person, 112 three-person, and 16 four-person interactions are annotated from all videos. In consultation with education experts, we further divide the two-person interactions into three categories (same row, left-back row v.s. right front row, right-back-row v.s. left-front row), and the three-person interactions into four categories (three people in the same row with two looking left, three people in the same row with two looking right, two people in the back row, two people in the front row)\footnote{It is observed by education experts that students' relative pose and positions strongly modulate their behaviors (\textit{e.g.} see \cite{Crouch:PI}) and consequently under their guidance we categorize the interactions based on students' relative pose and geometric configurations.}. Examples of these patterns are shown in Figure \ref{dataset}. The lengths of these occurrence intervals, meanwhile, range from a few seconds to tens of seconds. The 100 videos arise from five long-lectures, and we adopt a leave-one-lecture-out scheme in partitioning the training (database) exemplars and testing (input) videos. A separate dissimilarity measure is trained from each camera.


\begin{figure}[t]
\begin{center}
\includegraphics[scale=1.2]{dataset.png}
\end{center}
\caption{Samples of two-person ((a-1)(a-2)(a-3)), three-person ((b-1)(b-2)(b-3)(b-4)), and four-person (c) interactions in the classroom interaction dataset we collected.}
\label{dataset}
\end{figure}



A 4-level pyramid is set up at each ground-truth time interval, and a temporal unit is set at half length of the bottom-level cell. We use a coarse representation of the head pose as the individual activity descriptor. Specifically, we apply local translational alignment and size normalization to the bounding boxes with adjacent frames to compensate for imperfect tracking. Then, we compute the Histogram of Oriented Gradient (HOG) feature within each temporal unit and each box, and apply nine one-against-all SVMs to estimate the likelihood of a HOG feature belonging to nine head poses (front, left, lower-left, lower-front, lower-right, right, back-right, and back-left). The nine-dimensional likelihood vector eventually serves as our individual activity descriptor. On the other side, we derive the contextual descriptor for three or more individuals based on the geometrical configurations of the bounding boxes. As shown in the left panel of Figure \ref{all_illu}(b), where a descriptor of human $m$ in context of human $m'$ among five humans at time $t$ is computed, we compute the distances $r_{i}$'s between all others and $m$, and the relative angles $a_{i}$'s between the connecting vectors and $\overrightarrow{mm'}$, and combine all these geometric quantities into a contextual descriptor $\mathbf{g}_{m,m',t}$. When computing $\mathbf{g}_{n,n',s}$ in the input (right panel of Figure \ref{all_illu}(b)), we align $\overrightarrow{nn'}$ against $\overrightarrow{mm'}$ and predict the locations of the three individuals (shown in red), and compute the true distances $z_{i}$'s and relative angles $b_{i}$'s by locating the nearest individuals to the predicted locations. This contextual representation achieves invariance under similarity transforms. For two-person interaction, we simply use the distance and the relative angles against the right horizontal axis (Figure \ref{all_illu}(c)). 

\begin{table}[ht]
\centering \caption{Computational cost comparison for the proposed matching approach and baselines (in seconds).}
\footnotesize{
\begin{tabular}{|c|c|c|c|}
\hline    \# of Participants &  2  &  3  &  4   \\
\hline   Exhaustive+Sliding Window & 17.2   & 60.4   & 253.2   \\
\hline  Exhaustive+Branch and Bound &  12.6 &  27.6  &   59.7 \\
\hline  Optimal Pairing+Sliding Window & 12.4  & 23.2   &  40.8  \\
\hline  Proposed & 8.0  &  19.8  &  32.3  \\
\hline 
\end{tabular}
}
\label{computecost}
\end{table}


Before we look into the performance in correct detection and classification, we first look into the computational cost of the approach. We replace the optimal matching method with an exhaustive enumeration of all possible matchings. We also apply temporal sliding windows at different scales, and stop using the remaining scales whenever the current window achieves the same quality function value as the branch-and-bound, but we try at most eight scales ranging from half to twice of the exemplar length. We show the average computation time for one match between an exemplar and an input on a 8-core 2.8GHz Macintosh in Table \ref{computecost}, where we see clear savings for the proposed approach.


\begin{figure*}[t]
\begin{center}
\includegraphics[scale=2.5]{ROC.png}
\end{center}
\caption{ROC curves for identifying the participants of an two-person, three-person, and four-person interactions using the proposed approach and baselines. }
\label{ROC}
\end{figure*}

Note that we combine both individual activity (pose) descriptor and interaction (geometric context) descriptor and learn an optimal dissimilarity, we then evaluate the effect of this combination by using either descriptor (but not both) and identity matrices as the dissimilarity parameter. The ROC curves for detecting the participants of an interaction is shown in Figure \ref{ROC}. Note that using both descriptors and learning the optimal dissimilarity do yield the best performance, while it is interesting to see that contextual descriptor performs better than individual descriptor, mainly due to the fact that the interaction only occur among nearby students in a classroom environment and therefore geometric context provides a strong clue for a potential occurrence of interaction. Also note that increased number of participants achieves higher true positives against false positives. This phenomenon is expected, as when more contextual information is available from more humans, the true interaction pattern is more discriminative against the false ones.

\begin{figure*}[t]
\begin{center}
\includegraphics[scale=2.5]{classtemporal.png}
\end{center}
\caption{ Average classification accuracies and false positives for two-person and three-person interactions (Individual and/or contextual descriptors, with or without metric learning (ML)) and the temporal localization accuracies.}
\label{classtemporal}
\end{figure*}

The third evaluation is on the classification performance for the two-person and three-person interactions (as no further categorization for the four-person interactions is provided, no classification results on four-person interactions are shown). We examine those correctly detected pairs and triples and show the average true positive rates v.s. false positives when further classifying them into the three or four categories in Figure \ref{classtemporal}(a). Again note that either the dissimilarity learning or combining both descriptors gives rise to moderately improvement. As we are  at this stage investigating the correctly identified pairs or triples, the head poses serves a stronger evidence than spatial configuration for classification. A pose estimator, which is improved from current coarse representation and is more robust to videos of limited quality, will expect further improved classification performance.

\begin{table}[ht]
\centering \caption{Classification accuracies and false positive (FP) rates comparison on UT-Interaction dataset for evaluating the effectiveness of different components of the proposed approach: Individual and/or contextual descriptors, with or without metric learning (ML).}
\footnotesize{
\begin{tabular}{|c|c|c|c|}
\hline   & Individual only & Contextual only & Both \\
\hline Accur. w. ML & 0.688 & 0.813 & 0.854  \\
\hline Accur. w/o ML & 0.647 & 0.750 & 0.771    \\
\hline FP Rate w. ML &  0.125 & 0.096 & 0.071  \\
\hline FP Rate w/o ML & 0.163 & 0.113 & 0.083\\
\hline 
\end{tabular}
}
\label{UTaccuFPdegrade}
\end{table}


We finally investigate the temporal localization performance. For this purpose, we compute the ratio of the intersection to the union of the estimated interval and the annotated interval, and show the averages in Figure \ref{classtemporal}(b).  Note that the dissimilarity learning is designed in such a way as to encourage the group dissimilarity function to achieves negatives within an activity and positives otherwise. Therefore, the learned quality function of the branch-and-bound search more precisely reflects the boundaries of an activity.


\begin{figure*}
\begin{center}
\includegraphics[scale=2.25]{retrieved.png}
\end{center}
\vspace{-10pt}
\caption{Examples of social interaction detection and matching on the classroom interaction database. Each row is an example of detecting a salient interaction from an input and enumerating similar database exemplars: (a) is the input, (b) is the detected social interaction, and (c-1) to (c-3) are the top three associated database exemplars that support this detection.}
\label{retrieved}
\end{figure*}


As a visualization, Figure \ref{retrieved} shows examples of group interaction detection and matching on the classroom interaction database. Note that in the first and third row, a two-person interaction and a three-person interaction are correctly detected and associated with correct exemplars. In the second row, a false positive is detected (shown in blue dashed box), in which the two people are not interacting but demonstrating head poses that are similar to those in a conversation. In the fourth row, the three-person interaction (two looking to the left) is correctly detected but supported by an exemplar of different category (shown in blue dashed box, annotated as `two looking to the right'). Note that the head pose of person marked `2003' appears ambiguous between `looking to the left' and `looking to the right'. In the fifth row, a three-person interaction is correctly identified and associated with correct exemplars, though the head poses of the participants moderately vary among exemplars. 


\vspace{0.05in}

\noindent\textbf{UT-Interaction Dataset.} For a comparison with the state-of-art, we implement our approach on UT-Interaction dataset initially used in \cite{Ryoo:group} and recently again in \cite{Amer:group}. The dataset consists of 20 one-minute videos of continuous executions of 6 classes of two-person interactions: shaking-hands, pointing, hugging, pushing, kicking, and punching. Each video contains at least one instance of every interaction class, where distinct activities may occur at the same time. 10 videos are taken on a parking lot and the other 10 videos are captured in a natural setting by a moving camera. As the UT-Interaction dataset presents simultaneous performance of several activities, activities that may begin and end at arbitrary times, and the presence of people who are not involved in the activity, etc., it is well fitted into the scenario considered here. We follow exactly the same setup as in \cite{Ryoo:group,Amer:group}: 20\% of all available manual segmentations, each occupied by a unique activity instance, are used as `database exemplars' for training. The remaining full (unsegmented) sequences are used for testing. In training, a 4-level pyramid is set up at each ground-truth time interval, and a temporal unit is set at half length of the bottom-level cell. A 32-dimensional histogram of the spatio-temporal features developed by \cite{Dollar:STIP} in each unit and each bounding box is calculated first against a 500-word vocabulary from K-means clustering and then by PCA, and serves as the individual activity descriptor. Another 32-dimensional histogram of the optical flow computed from OpenCV in each unit and each box is calculated against 8 directions and 4 magnitudes, and the difference between the two histograms (relative motion) serves as the contextual descriptor between the two humans. In testing, we employ the human detector \cite{Pedro:detect}, and associate the detected boxes across frames to form continuous tracks of humans. 

\begin{table}[ht]
\centering \caption{Classification accuracies and false positive (FP) rates for the proposed method and the baselines on UT-Interaction dataset.}
\footnotesize{
\begin{tabular}{|c||c|c|}
\hline   & Accuracy (\cite{Ryoo:group}, \cite{Amer:group}, ours) & FP Rate (\cite{Ryoo:group}, \cite{Amer:group}, ours) \\
\hline Hug &  (0.875, 0.904, \underline{1.00}) & (0.075, 0.055, \underline{0.00}) \\
\hline Kick &  (0.750, 0.775, \underline{0.875})  & (0.138, 0.108, \underline{0.063})\\
\hline Point & (0.625, 0.663,  \underline{0.750}) & (\underline{0.025}, \underline{0.025}, 0.088)\\
\hline Punch & (0.500, 0.632, \underline{0.750})  & (0.201, 0.154,  \underline{0.138})\\
\hline Push & (0.750, \underline{0.782} , 0.750)  & (0.125, \underline{0.101},  0.138)\\
\hline Shake Hands &  (0.750, 0.789, \underline{1.00}) & (0.088, 0.060, \underline{0.00}) \\
\hline\hline Average &  (0.708, 0.758, \underline{0.854})  & (0.108, 0.083,  \underline{0.071})\\
\hline 
\end{tabular}
}
\label{UTaccuFP}
\end{table}


The first interesting investigation is again about the effectiveness of combining both individual descriptor and contextual descriptor, as well as metric learning for group dissimilarity function. We implement our approach with either but not both descriptor, and without learning the dissimilarity. The performance comparison is show in Table ~\ref{UTaccuFPdegrade}, which again proves the merit of the combined descriptors and learning optimal metrics between descriptors. It is interesting to see the contextual descriptor plays a more crucial role for this dataset: A significant performance drop arises when we only consider individual action descriptors. The next comparison is against the state-of-art methods \cite{Ryoo:group,Amer:group}. The recognition accuracies and false-alarm rates are shown in Table ~\ref{UTaccuFP}. In this evaluation, we only allow one database exemplar to produce a single response in an input, and claim a true positive only when both the class-label and the identified participants are simultaneously correct, otherwise a false positive is claimed for the exemplar class. We achieve improved accuracy and competitive false positive rate against the baselines. In the third evaluation, we specifically look into the temporal localization and participant identification performance. For temporal localization, we follow exactly the same criterion as in \cite{Amer:group}, requiring a true-positive to achieve both correct classification and a $>50\%$ ratio of the intersection to the union of the estimated interval and ground-truth. we achieve a slightly smaller area under ROC curve than the two baselines, as shown in Table ~\ref{UTarea}. Note that the temporal boundary is essentially ambiguous for these consecutively executed activities. For participant identification, we enforce an even stricter criterion to require $100\%$ correct identification (in contrast to $50\%$ in \cite{Amer:group}) for a true positive, and we outperform \cite{Amer:group}. Note that \cite{Ryoo:group} does not apply human detection and tracking and \cite{Amer:group} does not explicitly operate on the detected bounding boxes. Therefore, our performance should be regarded as an overall effect from both human detection/tracking and our group matching framework.

\begin{table}[ht]
\centering \caption{Area under ROC curve for the proposed method and the baselines on UT-Interaction dataset.}
\footnotesize{
\begin{tabular}{|c|c|c|c|}
\hline   & \cite{Ryoo:group} &  \cite{Amer:group}  &   ours \\
\hline Temporal Localization &  0.91 & \underline{0.94} &  0.89\\
\hline Participants Identification &  N/A & 0.87 &  \underline{0.93}   \\
\hline 
\end{tabular}
}
\label{UTarea}
\end{table}

\vspace{0.05in}

\noindent\textbf{Caltech Resident-Intruder Mouse Dataset.} We also tested the approach on Caltech Resident-Intruder Mouse Dataset \cite{CRIM13}, which contains long video sequences recording pair-wise interactions between two mice. Behaviors are categorized into 12 different mutually exclusive action types, plus an `other' category indicating no behavior of interest is occurring. A video typically lasts around 10 minutes at 25fps with a resolution of 640x480 pixels. Every video frame is labeled with one of the thirteen ground-truth categories, resulting in a segmentation of the videos
into action intervals. For more details please refer to \cite{CRIM13}. Note that in all videos are pair-wise interactions, and our experiment on this dataset is not meant to distinguish the participants, but to demonstrate that our approach can be directly used for a traditional task of temporal segmentation and classification without any changes.

We exactly follow the training/testing partitions provided by the dataset. We extract the spatio-temporal interest points (STIP) based appearance features and compute trajectory-based features from the tracks provided with the dataset as \cite{CRIM13} does (See \cite{CRIM13} for details). Differently from \cite{CRIM13}, we only compute STIP based features inside the bounding boxes enclosing the mice. The trajectory-based features consists of those describing the motion of each individual mouse and those describing the relative motion between two mice. We denote the former as T\_ind and the latter as T\_pair. Again we use a 4-level temporal pyramid, in which the STIP based appearance features are computed as \cite{CRIM13} does and the trajectory based features are also transformed into histograms against 64 codewords. 

Three combinations of features were attempted in \cite{CRIM13}: Trajectory only, STIP only, and using both. For the case trajectory only, our approach has two possible working modes: Using all trajectory-based features as individual descriptors (denoted as Trajectory\_1), or using T\_ind as individual descriptors and using T\_pair as contextual descriptors (denoted as Trajectory\_2). In the case of STIP only, all features are used as individual descriptors, and in the case of using both, STIP features and trajectory-based features serve as individual descriptor and contextual descriptors respectively. To classify a segment responded by one or more exemplars, we simply give it the label of the top-ranked exemplar. We follow the same error metric as \cite{CRIM13}, the frame-wise accuracy, and report the results in Table \ref{CRIMAccu}, where it is evident that motion trajectory is much more discriminative than local STIP based features given little articulation in a constrained environment. The competing performance by approach is achieved by splitting the motion into individual ones and contextual ones and learning separate metrics for them (Trajectory\_2). In other cases, multi-level adaboost based approach exhibits superiority on frame-wise classifications (as in \cite{CRIM13}).


\begin{table}[ht]
\centering \caption{Accuracies for the proposed method and the baselines on Caltech Resident-Intruder Mouse Dataset. (\%)}
\footnotesize{
\begin{tabular}{|c||c|c|c|c|}
\hline   & Trajectory\_1  & Trajectory\_2 & STIP & Both \\
\hline \cite{CRIM13} w/o. context & 52.3  & 52.3 & 29.3 & 53.1\\
\hline \cite{CRIM13} w. context &  \underline{58.3} & 58.3 & \underline{43.0} & 61.2\\
\hline Ours w/o. ML &  45.6 & 49.4 & 18.8 & 50.9 \\
\hline Ours w. ML & 54.5 & \underline{66.0} & 31.7 & \underline{62.9}\\
\hline 
\end{tabular}
}
\label{CRIMAccu}
\end{table}

It is observed in \cite{CRIM13} that accuracy varies with the length of the interaction and with the length of window within which the local feature is computed. To investigate the performance of our approach on different lengths of the interaction as compared to \cite{CRIM13}, we implemented the approach in \cite{CRIM13} using trajectory-based features and one-level auto-context classifier. As shown by the result in Figure \ref{frame_no}, our approach is particularly better at localizing longer interactions though \cite{CRIM13} demonstrates its advantage under a shorter feature window on shorter interactions. 

\begin{figure}[t]
\begin{center}
\includegraphics[scale=0.25]{frame_no.png}
\end{center}
\caption{Accuracy comparison for varying length of interactions between \cite{CRIM13} and our approach.}
\label{frame_no}
\end{figure}

\vspace{0.05in}

\noindent\textbf{Conclusion.} We have proposed an approach to find small-group social interactions in space and time among a larger group of people. We achieve simultaneous participant identification, temporal localization, and classification. These functionalities are enabled and optimized by learning the best ensemble (dis)similarities from data. Our approach is flexible and generic to various of applications provided that an individual behavior descriptor and a contextual behavior descriptor are properly defined and computed. On the other hand, a further improvement of the overall performance will also rely on the robustness and discrimination of these descriptors, especially when videos are of limited quality such as the classroom videos we use. 

\bibliographystyle{ieee}
{\footnotesize
\bibliography{egbib}
}




\end{document}
