% !TEX root =  GroupDet.tex

\begin{abstract}
We consider the problem of finding distinctive social interactions involving groups of people embedded within larger social gatherings. Given a pre-defined gallery of short exemplar interaction videos, and a long input video of a large gathering (with approximately-tracked actors), we identify within the gathering small sub-groups of people exhibiting social interactions that resemble those in the exemplars. The participants of each detected group interaction are localized in space; the extent of their interaction is localized in time; and when the gallery of exemplars is annotated with group-interaction categories, each detected interaction is  classified into one of the pre-defined categories. Our approach represents group behaviors by dichotomous collections of descriptors for (i) individual actions, and (ii) pair-wise interactions; and it includes an efficient algorithm for optimally distinguishing participants from bystanders in every temporal frame. Participant information is accumulated over time through a voting scheme that provides insensitivity to tracking errors (e.g., broken tracks, false detections) as well as variations in the rate and temporal extent of interaction. Most importantly, the method is general and can be applied whenever numerous interacting agents can be approximately tracked over time. We evaluate the approach using three different video collections, two that involve humans and one that involves mice.
\end{abstract}