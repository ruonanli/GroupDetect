\section{Descriptor Metric Learning}
\label{MetLearn}

We learn each pair of Mahalanobis metrics $(\Sigma_{I}, \Sigma_{P})$ for each number of participants separately. To do so, we construct two different types of collections from our database exemplars. The first collection, $\mathcal{P}$, contains all pairs of instantaneous interaction ensembles that are similar to each other. The second collection, $\mathcal{M}$, is comprised of all ordered triples $(h,k,l)$ in which ensemble $h$ is similar to ensemble $k$ but they two  are dissimilar to ensemble $l$. In the present case, we build the collection $\mathcal{P}$ and the ($(h,k)$-indexed) similar pairs in collection $\mathcal{M}$ using all instantaneous ensemble pairs with their ground-truth matching matrices from the same category extracted from the same cell at the lowest level of the pyramid. Similarly, we include in the ($(h,l)$-indexed and $(k,l)$-indexed) dissimilar pairs of collection $\mathcal{M}$ the ensembles extracted from: 1) different-category exemplars; 2) same-category same-cell ensembles with simulated wrong matching matrices; and 3) ensembles within interaction intervals against ensembles from `background' non-interaction intervals. Eventually, we find the Mahalanobis metrics by solving
\begin{equation}
\label{classify}
\begin{split}
&\min_{\xi_{h,k,l}\ge0, \Sigma_{I}\succeq 0, \Sigma_{P}\succeq 0} \sum_{(u,v)\in\mathcal{P}}\hat{D}(\mathcal{D}_{u}, \mathcal{D}_{v}, W_{u,v})+\gamma\sum_{(h,k,l)\in\mathcal{M}}\xi_{h,k,l}\\
&\textup{s.t.}  \hat{D}(\mathcal{D}_{h}, \mathcal{D}_{l}, W_{h,l})-\hat{D}(\mathcal{D}_{h}, \mathcal{D}_{k}, W_{h,k})\ge 2\Delta-\xi_{h,k,l}.
\end{split}
\end{equation}
Note that $\Delta$ is the large positive number as used in (\ref{quality}), and (\ref{classify}) will encourage a similar pair to contribute a negative summand around $-\Delta$ to (\ref{quality}), and encourage a dissimilar pair to contribute approximately $+\Delta$ summand,  thus enabling the branch-and-bound search as introduced in Section \ref{BB}.  The minimization over either $\Sigma_{I}$ or $\Sigma_{P}$ falls into the framework of large margin nearest neighbor (LMNN) formulation \cite{Weinberger:ML}, and we simply decompose (\ref{classify}) into independent LMNN tasks and employ \cite{Weinberger:ML}.


\ruonan{Please consider add back some of the argument commented below}.


%Recall that 1) in Section \ref{vote} we expect the score $D$ to be small when a temporal unit in the input is best aligned to a temporal unit in the exemplar by the `correct' matching matrix; and 2) in Section \ref{BB} we expect the dissimilarity $D^{*}(t)$ are driven toward zero in the `ground-truth' interval $[t_{s}, t_{e}]$ , and toward a large positive number $2\Delta$ otherwise. For both purposes, we learn an effective ensemble dissimilarity measure. Recall that the ensemble dissimilarity $D$ or $\hat{D}$ is aggregated from `atomic' descriptor distances $\{ d_{I}(\mathbf{f}_{m,t}, \mathbf{f}^{D}_{n,s}), d_{P}(\mathbf{g}_{m,m',t}, \mathbf{g}^{D}_{n,n',s})\}$, and the ensemble dissimilarity learning therefore narrows down to learning effective `atomic'  distances.  We parameterize an atomic distance by a Mahalanobis metric, \textit{i.e.}, let $d_{I}(\mathbf{f}, \mathbf{f}')=(\mathbf{f}-\mathbf{f}')^{T}\Sigma_{I}(\mathbf{f}-\mathbf{f}')$ and $d_{P}(\mathbf{g}, \mathbf{g}')=(\mathbf{g}-\mathbf{g}')^{T}\Sigma_{P}(\mathbf{g}-\mathbf{g}')$, where $\Sigma_{I}\succeq 0$ and $\Sigma_{P}\succeq 0$ are positive semi-definite matrices.  
%
%We learn each pair of Mahalanobis metrics $(\Sigma_{I}, \Sigma_{P})$ for each number of participants separately. To do so, we construct two different types of collections from our database exemplars. The first collection, $\mathcal{P}$, contains all pairs of instantaneous interaction ensembles that are similar to each other. The second collection, $\mathcal{M}$, is comprised of all ordered triples $(h,k,l)$ in which ensemble $h$ is similar to ensemble $k$ but they two  are dissimilar to ensemble $l$. In the present case, we build the collection $\mathcal{P}$ and the ($(h,k)$-indexed) similar pairs in collection $\mathcal{M}$ using all instantaneous ensemble pairs with their ground-truth matching matrices from the same category extracted from the same cell at the lowest level of the pyramid. Similarly, we include in the ($(h,l)$-indexed and $(k,l)$-indexed) dissimilar pairs of collection $\mathcal{M}$ the ensembles extracted from: 1) different-category exemplars; 2) same-category same-cell ensembles with simulated wrong matching matrices; and 3) ensembles within interaction intervals against ensembles from `background' non-interaction intervals. Eventually, we find the Mahalanobis metrics by solving
%\begin{equation}
%\label{classify}
%\begin{split}
%&\min_{\xi_{h,k,l}\ge0, \Sigma_{I}\succeq 0, \Sigma_{P}\succeq 0} \sum_{(u,v)\in\mathcal{P}}\hat{D}(\mathcal{D}_{u}, \mathcal{D}_{v}, W_{u,v})+\gamma\sum_{(h,k,l)\in\mathcal{M}}\xi_{h,k,l}\\
%&\textup{s.t.}  \hat{D}(\mathcal{D}_{h}, \mathcal{D}_{l}, W_{h,l})-\hat{D}(\mathcal{D}_{h}, \mathcal{D}_{k}, W_{h,k})\ge 2\Delta-\xi_{h,k,l}.
%\end{split}
%\end{equation}
%Note that $\Delta$ is the large positive number as used in (\ref{quality}), and (\ref{classify}) will encourage a similar pair to contribute a negative summand around $-\Delta$ to (\ref{quality}), and encourage a dissimilar pair to contribute approximately $+\Delta$ summand,  thus enabling the branch-and-bound search as introduced in Section \ref{BB}.  The minimization over either $\Sigma_{I}$ or $\Sigma_{P}$ falls into the framework of large margin nearest neighbor (LMNN) formulation \cite{Weinberger:ML}, and we simply decompose (\ref{classify}) into independent LMNN tasks and employ \cite{Weinberger:ML}.
