% !TEX root =  GroupDet.tex

\section{Descriptor Metric Learning}
\label{MetLearn}
\vspace{-5pt}
As mentioned in Sec. \ref{agg}, we learn matrices $\Sigma_{I}$, $\Sigma_{P}$ for the Mahalanobis distances $d_I(\mathbf{f},\mathbf{f}')$ and $d_P(\mathbf{g},\mathbf{g}')$, so that the learned metrics can: 1) enhance discrimination between exemplar categories by ensuring that distances are smaller when descriptors are drawn from roughly the same temporal location within a labeled exemplar of the same category, and larger otherwise; and 2) enhance the accuracy of temporal localization by ensuring that distances between labeled ensembles and unlabeled ``background'' ensembles are large. Both 1) and 2) lead to more accurate spatial localizations of participants (\ie better $W^{*}$ as discussed in Sec. \ref{vote}), and they induce the ``quality function'' conditions required for efficient temporal localization by branch-and-bound (Sec.~\ref{BB}). We achieve all of these benefits simultaneously by using an adaptation of the Large Margin Nearest Neighbor (LMNN) framework~\cite{Weinberger:ML}. 

For each application scenario, we use a training set of exemplar videos---possibly having varying numbers of agents $N$---that are annotated with start/end times, category labels, $N$-agent correspondences between exemplars of the same category. We use unlabeled time units in the videos as ``background'' samples. Intuitively, the learned metrics should satisfy the six types of constraints shown in Fig.~\ref{ML_illustration}. This figure depicts three different exemplar videos in which a subset of time units have been labeled as being distinctive interactions of two different classes. In this example, each labeled exemplar is shown as being divided into two cells; these correspond to the lowest level of the temporal pyramid described in Sec.~\ref{BB}. The first three constraints in the list enhance discrimination between categories, while the last three enhance the accuracy of temporal localization. Offsetting all of the distances by $-1$ ensures that the function $q(t)$ in (\ref{qfunction}) assumes proper negative values as required for branch-and-bound search. 

\begin{figure}[t]
\vspace{-5pt}
\begin{centering}
\includegraphics[width=\columnwidth]{MetricLearning_new}
\end{centering}
\caption{Constraints used in discriminative metric learning. Each row is an annotated two-cell exemplar with markers representing instantaneous descriptor-ensembles at each time unit. For discrimination between interaction categories, distances between ensembles of the same class (red circles and red squares) should be small whenever they occur in the same cell number; and distances for different classes (red vs. yellow) should be large. For effective and efficient temporal localization, distances between ensembles at labeled times and unlabeled ``background'' times (black circles) should be large, and all distances should be offset by $-1$. See text for details.}
\label{ML_illustration}
\vspace{-5pt}
\end{figure}

We enforce these constraints through an LMNN framework by construct two collections from our database exemplars. The collection $\mathcal{P}$ contains all pairs of instantaneous interaction ensembles that are of the same category (red circles and red squares in Fig.~\ref{ML_illustration}) and occur roughly in the same temporal location within the interaction instances (\ie, in the same cell of the lowest level of the temporal pyramids), together with their ``ground-truth" matchings. The collection $\mathcal{M}$ is comprised of ordered triples $(h,k,l)$ in which ensemble $h$ is the same category as ensemble $k$ and ensemble $l$ is either of a different category or background. Having defined these two collections, each Mahalanobis metric is found by solving
\begin{equation}
\vspace{-5pt}
\label{classify}
\begin{split}
&\min_{\Sigma_{I}\succeq 0, \Sigma_{P}\succeq 0} \sum_{(u,v)\in\mathcal{P}}\hat{D}(\mathcal{D}_{u}, \mathcal{D}_{v}, W_{u,v})+\gamma\sum_{(h,k,l)\in\mathcal{M}}\xi_{h,k,l}\\
&\textup{s.t.}  D(\mathcal{D}_{h}, \mathcal{D}_{l})-\hat{D}(\mathcal{D}_{h}, \mathcal{D}_{k}, W_{h,k})\ge 2-\xi_{h,k,l}, \xi_{h,k,l}\ge0,
\end{split}
\end{equation}
where $W_{u,v}$ is the ``ground-truth" matching for pair $(u,v)$\footnote{It is useful to add to the collection $\mathcal{M}$ additional triples in which $l$ is derived from same-category same-cell pairs but with permuted incorrect matching matrices. In this case $D(\mathcal{D}_{h}, \mathcal{D}_{l})$ in (\ref{classify}) is replaced by $\hat{D}(\mathcal{D}_{h}, \mathcal{D}_{l}, \bar{W}_{h,l})$, where $\bar{W}_{h,l}$ is the permuted incorrect matching matrix.}. The minimization over either $\Sigma_{I}$ or $\Sigma_{P}$ is exactly a LMNN problem\cite{Weinberger:ML}, and we apply LMNN multiple times to separately learn one distinct pair of ($\Sigma_{I}, \Sigma_{P}$) for each value of $N$ that exists in the training set. 


%Recall that 1) in Section \ref{vote} we expect the score $D$ to be small when a temporal unit in the input is best aligned to a temporal unit in the exemplar by the `correct' matching matrix; and 2) in Section \ref{BB} we expect the dissimilarity $D^{*}(t)$ are driven toward zero in the `ground-truth' interval $[t_{s}, t_{e}]$ , and toward a large positive number $2\Delta$ otherwise. For both purposes, we learn an effective ensemble dissimilarity measure. Recall that the ensemble dissimilarity $D$ or $\hat{D}$ is aggregated from `atomic' descriptor distances $\{ d_{I}(\mathbf{f}_{m,t}, \mathbf{f}^{D}_{n,s}), d_{P}(\mathbf{g}_{m,m',t}, \mathbf{g}^{D}_{n,n',s})\}$, and the ensemble dissimilarity learning therefore narrows down to learning effective `atomic'  distances.  We parameterize an atomic distance by a Mahalanobis metric, \textit{i.e.}, let $d_{I}(\mathbf{f}, \mathbf{f}')=(\mathbf{f}-\mathbf{f}')^{T}\Sigma_{I}(\mathbf{f}-\mathbf{f}')$ and $d_{P}(\mathbf{g}, \mathbf{g}')=(\mathbf{g}-\mathbf{g}')^{T}\Sigma_{P}(\mathbf{g}-\mathbf{g}')$, where $\Sigma_{I}\succeq 0$ and $\Sigma_{P}\succeq 0$ are positive semi-definite matrices.  
%
%We learn each pair of Mahalanobis metrics $(\Sigma_{I}, \Sigma_{P})$ for each number of participants separately. To do so, we construct two different types of collections from our database exemplars. The first collection, $\mathcal{P}$, contains all pairs of instantaneous interaction ensembles that are similar to each other. The second collection, $\mathcal{M}$, is comprised of all ordered triples $(h,k,l)$ in which ensemble $h$ is similar to ensemble $k$ but they two  are dissimilar to ensemble $l$. In the present case, we build the collection $\mathcal{P}$ and the ($(h,k)$-indexed) similar pairs in collection $\mathcal{M}$ using all instantaneous ensemble pairs with their ground-truth matching matrices from the same category extracted from the same cell at the lowest level of the pyramid. Similarly, we include in the ($(h,l)$-indexed and $(k,l)$-indexed) dissimilar pairs of collection $\mathcal{M}$ the ensembles extracted from: 1) different-category exemplars; 2) same-category same-cell ensembles with simulated wrong matching matrices; and 3) ensembles within interaction intervals against ensembles from `background' non-interaction intervals. Eventually, we find the Mahalanobis metrics by solving
%\begin{equation}
%\label{classify}
%\begin{split}
%&\min_{\xi_{h,k,l}\ge0, \Sigma_{I}\succeq 0, \Sigma_{P}\succeq 0} \sum_{(u,v)\in\mathcal{P}}\hat{D}(\mathcal{D}_{u}, \mathcal{D}_{v}, W_{u,v})+\gamma\sum_{(h,k,l)\in\mathcal{M}}\xi_{h,k,l}\\
%&\textup{s.t.}  \hat{D}(\mathcal{D}_{h}, \mathcal{D}_{l}, W_{h,l})-\hat{D}(\mathcal{D}_{h}, \mathcal{D}_{k}, W_{h,k})\ge 2\Delta-\xi_{h,k,l}.
%\end{split}
%\end{equation}
%Note that $\Delta$ is the large positive number as used in (\ref{quality}), and (\ref{classify}) will encourage a similar pair to contribute a negative summand around $-\Delta$ to (\ref{quality}), and encourage a dissimilar pair to contribute approximately $+\Delta$ summand,  thus enabling the branch-and-bound search as introduced in Section \ref{BB}.  The minimization over either $\Sigma_{I}$ or $\Sigma_{P}$ falls into the framework of large margin nearest neighbor (LMNN) formulation \cite{Weinberger:ML}, and we simply decompose (\ref{classify}) into independent LMNN tasks and employ \cite{Weinberger:ML}.
